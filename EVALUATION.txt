------------------------------------
| REGISTER ALLOCATION OPTIMIZATION |
------------------------------------

1. Scope

This implementation does not perform register allocation for global variables
and array locations. Moreover, I use a single live range per variable.

Coalescence is performed, but I figured it is already covered by the given
algorithm. At least for the single live range per variable scenario.

2. Registers used

In this implementation I use a set of 16 registers for variable allocation:
$t2 - $t10
$s0 - $s7

Registers $t0 and $t1 are reserved for intermediary operations and are never
used for long-term storage of a live range.

3. Caller/callee saving procedure

Registers from $t2 to $t10 are saved and restored by the caller before and
right after a function call. On the other hand, registers from $s0 to $s7 are
written to memory by the callee in its ENTER and restored back in this LEAVE
instructions.

When I first executed this optimization, the aforementioned procedure actually
increased the number of writes and reads from memory because of the
indiscriminate savings and loadings to and from memory at every function call.
To reduce the number of write and load operations, I keep a set of registers
used in a function and I only save and restore registers that are used inside
that function.

If a function being called has not been parsed yet (function definition delayed)
, I make no assumptions and I save all the registers that will be used in the
caller later (registers reserved for variables live after the function call)
before invoking the callee. In most of the cases, function definitions will be
 given before they are called by another function, therefore, this issue
 shouldn't happen often.

Also, I made sure that the set of registers used by a function is updated in
such a way that in the end it also includes any registers used by other
functions invoked within the former function's scope.

Finally, since the println function was hardcoded and no $t register is used in
it, I manually defined the set of registers used in that function to empty.

3. Choosing registers to allocate

I made a slight modification to the original algorithms to reduce register
clashing. Before I describe them, consider the following definitions

- Node: A variable's live range (sometimes I will abuse this definition and I
will use node and variable interchangeably. Hopefully, it will be clear from the
 context whenever I do so).
- AvoidSet: Set of registers that cannot be used by a node. Every node in the
graph has its own AvoidSet.
- PrefSet: Set of registers that should be given preference by a node. Every
node in the graph has its own PrefSet.

a. Constructing the interference graph

1. Initialize the PrefSet of each node with a full set of registers.
2. Proceed with the original algorithm.
3. Whenever a CALL instruction is processed, for every variable in LiveNow set
at that point, remove the registers used in the function being called from
those variables' PrefSet.

In the end, the PrefSet for each node will contain registers that are not used
by functions called when the node's variable is live. If we can allocate
some nodes to one of the registers in their PrefSet, this will reduce the number
 of caller saves/loads needed at function calls.

As it will be clear next, even if this PrefSet for a node is not empty, it's not
guaranteed that we will be able to use a register from it for that node.

b. Coloring the graph

Let K be the max number of registers to use and RSet be the set of all K
registers.

1. Initialize the AvoidSet of each node with an empty set of registers.
2. REPEAT
3.   get node with highest cost from the list of nodes with < K neighbors
4.   remove node from the graph
5.   IF node's PrefSet - nodes's AvoidSet is empty THEN
6.	 	AvailableSet = RSet - nodes's AvoidSet
7.   ELSE
8.      AvailableSet = node's PrefSet - nodes's AvoidSet
9.   END
10.  assign any register from AvailableSet to the node
11.  add the chosen register to the AvoidSet of every neighbor of the node
12.  UNTIL
13.  ... // Equals to the original algorithm

Let's go over the modifications introduced to understand the intuition behind
them.

In line 3, instead of picking an arbitrary node with fewer than K neighbors
from the graph, I choose the one with highest cost to increase the likelihood of
 being able to allocate high cost variables to one of the registers in their
 PrefSet. Notice that as I color the graph, I include elements to some nodes'
 AvoidSet. If we delay the evaluation of a node, we might end up with a
 situation where all the registers in its PrefSet are also in its AvoidSet (line
  6), forcing us to pick a register that will be stored/retrieved from memory by
   a the caller at some point. Processing the nodes with higher cost first will
   reduce the amount of times this issue happens with high cost variables.

Finally, line 11 is necessary so we don't end up allocating the same register
for two neighbor live ranges.

At the end of this algorithm, the variables will already be informed about the
registers they must use in the generated assembly code.


4. Cost of a variable

I compute the cost of a variable during my code generation phase. Therefore,
over the AST. I used the following atomic instruction costs:

- Cost at the AST root: 1
- Cost in a single instruction: 1
- Cost in a branch: 1
- Cost in a loop initialization: 1
- Cost in a loop update and evaluation: 10
- Cost in a loop body: 10

The cost is processed recursively, thus variables in a nested scope will yield
the cost of the outer scope multiplied by the incurring cost of the instructions
 they are in in the inner scope.

5. Type conversion

TODO xxxx

--------------------------
| ORDER OF OPTIMIZATIONS |
--------------------------

I process register allocation after local and global optimization. This is a
different kind of optimization as it does not work by removing intermediary
instructions but machine code. Eliminating intermediary instructions first, by
the local and global optimization procedures, might reduce the number of
registers actually needed, improving the overall performance of the compiler.

--------------------------
| IMPLEMENTATION DETAILS |
--------------------------

Not to increase too much the compilation overhead, I implemented the set of
nodes to color and to spill as a max and min heap respectively. Therefore,
getting a node with highest/lowest cost will be O(logn) instead of O(n) which is
 what would get if I worked with a linked list.

---------------
| Experiments |
---------------

The programs used in this section are in the directory eval/a3.

- Program 1

This program explores the effect of peephole on code optimization. It contains
each one of the instructions that can be optimized by the peephole approach.
By suppressing copy propagation, we can see that all credits on the reduction
of number of write and branch instructions goes to peephole in this
sample program. Here are the results reported by spim before and after
optimization:

Before Optimization
Stats -- #instructions : 178
         #reads : 60  #writes 47  #branches 17  #other 54

After Optimization (peephole only)
Stats -- #instructions : 153
         #reads : 48  #writes 35  #branches 16  #other 54

After Optimization (peephole only + copy propagation)
Stats -- #instructions : 153
         #reads : 27  #writes 35  #branches 16  #other 75

Considering only peephole: 20% reduction in the number of read instructions and
25% reduction in the number of write instructions due to optimizations c and d.
And 5% reduction in the number of branch instructions due to optimizations a and
 b.

- Program 2

This programs explores the effect of copy propagation on code optimization. It
contains a series of consecutive assignment instructions that can be propagated
by transitivity. Discounting the single write instruction optimized by peephole
(optimization d), the expressive reduction in the number of read instructions
is all due to copy propagation.

Before Optimization
Stats -- #instructions : 61
         #reads : 23  #writes 23  #branches 3  #other 12

After Optimization
Stats -- #instructions : 59
         #reads : 3  #writes 22  #branches 3  #other 31

87% reduction in the number of read instructions.

- Program 3

This programs explores the effect of dead code elimination on a program with
three blocks comprised of instructions that are all dead.

Before Optimization
Stats -- #instructions : 85
         #reads : 25  #writes 25  #branches 7  #other 28

After Optimization
Stats -- #instructions : 47
         #reads : 7  #writes 6  #branches 7  #other 27

72% reduction in the number of read instructions and 76% reduction in the number
 of write instructions. The resultant optimized code does not contain any
 assignment instructions but the ones necessary to assign a constant to a
 temporary variable that is, in turn, used as parameter to the println function
  call.

- Program 4

This program is a merge of the previous 3 programs. I aim to explore the
contribution of each optimization individually and as a whole.

Before Optimization
Stats -- #instructions : 318
         #reads : 113  #writes 102  #branches 23  #other 80

After Optimization (peephole only)
Stats -- #instructions : 271
         #reads : 90  #writes 79  #branches 22  #other 80

After Optimization (peephole + copy propagation)
Stats -- #instructions : 271
         #reads : 42  #writes 79  #branches 22  #other 128

After Optimization (dead code elimination only)
Stats -- #instructions : 218
         #reads : 65  #writes 52  #branches 23  #other 78

After Optimization(local + global optimization)
Stats -- #instructions : 145
         #reads : 29  #writes 16  #branches 22  #other 78


The more we use combined optimization methods, the more instructions we can
discard in the code. One thing to notice here is the effect of local
optimization followed by a global one. The copy propagation process created some
 dead instructions that could be later removed by dead code elimination,
 causing a reduction of 65% in the number of read instructions (from 65 to 29)
 and 70% in the number of dead instructions (from 52 to 16) compared to the
 number of read/write instructions after dead code elimination alone.

- Program 5

The most costly of the three algorithms implemented is dead code elimination
because it requires liveness analysis and the information is not constrained to
a single block. This program consists of a series of dead instructions
inside nested loops (therefore, multiple blocks) to allow us to compare how the
optimization affects the running time of the compiler.

Before Optimization
Stats -- #instructions : 110571
         #reads : 38902  #writes 36855  #branches 10238  #other 24576

Running time (10 independent executions): 0.21 +|- 0.035 ms

After Optimization(local + global optimization)
Stats -- #instructions : 47100
         #reads : 10238  #writes 6143  #branches 8191  #other 22528

Running time (10 independent executions): 1 +|- 0.04 ms

Local and global optimizations were able to reduce by a large amount the number
of instructions in this sample program. 76% reduction in read, 83% in write and
20% branch instructions. All of this, however, at a cost of taking almost 5x
more milliseconds to generate the final assembly code.









